name: Enhanced Radio Scraper Cron
on:
  schedule:
    # Run every 4 hours for more frequent data
    - cron: '0 */4 * * *'
  workflow_dispatch:
    inputs:
      force_full_scrape:
        description: 'Force full scrape (all stations)'
        required: false
        default: 'false'
        type: boolean
      debug_mode:
        description: 'Enable debug logging'
        required: false
        default: 'false'
        type: boolean

# Prevent concurrent runs to avoid data corruption
concurrency:
  group: radio-scraper-${{ github.ref }}
  cancel-in-progress: true

jobs:
  validate-environment:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      secrets_valid: ${{ steps.check-secrets.outputs.valid }}
      api_reachable: ${{ steps.check-api.outputs.reachable }}
    
    steps:
      - name: Check required secrets exist
        id: check-secrets
        run: |
          REQUIRED_SECRETS=("RENDER_API_URL" "SCRAPER_API_KEY" "INTERNAL_TOKEN")
          missing_secrets=()
          
          for secret in "${REQUIRED_SECRETS[@]}"; do
            if [ -z "${{ secrets[secret] }}" ]; then
              missing_secrets+=("$secret")
            fi
          done
          
          if [ ${#missing_secrets[@]} -eq 0 ]; then
            echo "âœ… All secrets present"
            echo "valid=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ Missing secrets: ${missing_secrets[*]}"
            echo "valid=false" >> $GITHUB_OUTPUT
          fi
      
      - name: Verify API is reachable
        if: steps.check-secrets.outputs.valid == 'true'
        id: check-api
        run: |
          API_URL="${{ secrets.RENDER_API_URL }}"
          if curl -s --max-time 10 "$API_URL/health" | grep -q "healthy"; then
            echo "âœ… API is reachable"
            echo "reachable=true" >> $GITHUB_OUTPUT
          else
            echo "âŒ API is not reachable"
            echo "reachable=false" >> $GITHUB_OUTPUT
          fi

  scrape:
    needs: validate-environment
    if: needs.validate-environment.outputs.secrets_valid == 'true' && needs.validate-environment.outputs.api_reachable == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 30  # Prevent infinite runs
    
    strategy:
      fail-fast: false
      max-parallel: 1
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
      
      - name: Setup Python with caching
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          # Additional scraper-specific dependencies
          pip install aiohttp requests beautifulsoup4 lxml
      
      - name: Run enhanced scraper with monitoring
        id: scraper
        env:
          API_URL: ${{ secrets.RENDER_API_URL }}
          INTERNAL_TOKEN: ${{ secrets.INTERNAL_TOKEN }}
          FORCE_FULL_SCRAPE: ${{ github.event.inputs.force_full_scrape || 'false' }}
          DEBUG_MODE: ${{ github.event.inputs.debug_mode || 'false' }}
        run: |
          set -e  # Exit on error
          
          START_TIME=$(date +%s)
          
          echo "ðŸš€ Starting enhanced radio scraper..."
          echo "ðŸ“… $(date)"
          echo "ðŸŒ API: $API_URL"
          echo "ðŸ”§ Mode: $FORCE_FULL_SCRAPE"
          echo "ðŸ› Debug: $DEBUG_MODE"
          
          # Create enhanced scraper directory
          mkdir -p .scraper_logs
          
          # Run scraper with tee to capture output
          python scripts/enhanced_radio_scraper.py 2>&1 | tee .scraper_logs/run_$(date +%Y%m%d_%H%M%S).log
          
          SCRAPER_EXIT_CODE=${PIPESTATUS[0]}
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo "::set-output name=exit_code::$SCRAPER_EXIT_CODE"
          echo "::set-output name=duration::$DURATION"
          
          if [ $SCRAPER_EXIT_CODE -eq 0 ]; then
            echo "âœ… Scraper completed successfully"
            echo "â±ï¸ Duration: ${DURATION}s"
          else
            echo "âŒ Scraper failed with code: $SCRAPER_EXIT_CODE"
            echo "â±ï¸ Duration: ${DURATION}s"
            exit $SCRAPER_EXIT_CODE
          fi
      
      - name: Upload scraper logs and artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs-${{ github.run_id }}
          path: |
            .scraper_logs/
            scripts/*.py
          retention-days: 30
      
      - name: Generate performance report
        if: always()
        run: |
          echo "## ðŸ“Š Scraper Performance Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run Details:**" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ“… Date: $(date)" >> $GITHUB_STEP_SUMMARY
          echo "- â±ï¸ Duration: ${{ steps.scraper.outputs.duration || 'Unknown' }} seconds" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸ”§ Scraper Version: $(cd scripts && git log -1 --oneline)" >> $GITHUB_STEP_SUMMARY
          echo "- ðŸƒ Runner: ${{ runner.os }} ${{ runner.arch }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.scraper.outputs.exit_code }}" = "0" ]; then
            echo "âœ… **SUCCESS** - Scraper completed successfully" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **FAILURE** - Scraper exited with code: ${{ steps.scraper.outputs.exit_code }}" >> $GITHUB_STEP_SUMMARY
          fi
      
      - name: Send notification on failure
        if: failure() && steps.scraper.outputs.exit_code != '0'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `ðŸš¨ **Radio Scraper Failed**\n\nWorkflow ${context.runId} failed with exit code ${{ steps.scraper.outputs.exit_code }}.\n\n[View Logs](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`
            })

  keepalive:
    needs: scrape
    if: always() && github.event_name == 'schedule'
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
      - name: Keepalive ping
        uses: gautamkrishnar/keepalive-workflow@v1
        with:
          committer_username: 'github-actions[bot]'
          committer_email: 'github-actions[bot]@users.noreply.github.com'
          commit_message: 'ðŸ¤– Automated keepalive commit for radio scraper'
          force_push: false
