name: Ugandan Radio Scraper
on:
  # Runs every 30 minutes
  schedule:
    - cron: '*/30 * * * *'
  # Manual trigger from GitHub UI
  workflow_dispatch:
  # Trigger when you push to main (for testing)
  push:
    branches: [main]
    paths: ['**.py', '.github/workflows/radio-scraper.yml']

jobs:
  scrape-and-send:
    runs-on: ubuntu-latest
    timeout-minutes: 10  # Prevents infinite runs
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        pip install aiohttp
        pip install requests
        
    - name: Run radio scraper
      id: scrape
      run: |
        echo "üïê $(date) - Starting radio scrape..."
        
        # Create the scraper script
        cat > radio_scraper.py << 'EOF'
import asyncio
import aiohttp
import re
from datetime import datetime
import hashlib
import json
import os
import sys

class UgandaRadioScraper:
    """Scraper for Ugandan radio stations"""
    
    def __init__(self):
        self.stations = [
            {
                "name": "Capital FM",
                "url": "https://ice.capitalradio.co.ug/capital_live",
                "region": "Eastern",
                "frequency": "91.3"
            },
            {
                "name": "Galaxy FM", 
                "url": "http://41.210.160.10:8000/stream",
                "region": "Eastern",
                "frequency": "100.2"
            },
            {
                "name": "Radio Simba",
                "url": "https://stream.radiosimba.ug/live", 
                "region": "Eastern",
                "frequency": "97.3"
            },
            {
                "name": "Beat FM",
                "url": "http://91.193.183.197:8000/beatfm",
                "region": "Eastern",
                "frequency": "96.3"
            },
            {
                "name": "Crooze FM",
                "url": "http://stream.croozefm.com:8000/croozefm",
                "region": "Western",
                "frequency": "91.2"
            }
        ]
    
    async def scrape_one(self, station):
        """Scrape one station"""
        try:
            headers = {
                'Icy-MetaData': '1', 
                'User-Agent': 'UG-Board-Scraper/1.0 (GitHub Actions)'
            }
            
            timeout = aiohttp.ClientTimeout(total=10)
            
            async with aiohttp.ClientSession(timeout=timeout) as session:
                async with session.get(station["url"], headers=headers) as response:
                    if response.status != 200:
                        print(f"‚ùå {station['name']}: HTTP {response.status}")
                        return None
                    
                    metaint = int(response.headers.get('icy-metaint', 0))
                    if metaint == 0:
                        print(f"‚ö†Ô∏è {station['name']}: No metadata support")
                        return None
                    
                    # Read audio data until metadata
                    reader = response.content
                    await reader.readexactly(metaint)
                    
                    # Read metadata length
                    meta_byte = await reader.readexactly(1)
                    meta_length = ord(meta_byte) * 16
                    
                    if meta_length == 0:
                        return None
                    
                    # Read metadata
                    meta_data = await reader.readexactly(meta_length)
                    meta_text = meta_data.decode('utf-8', errors='ignore')
                    
                    # Find song title
                    match = re.search(r"StreamTitle='(.*?)';", meta_text)
                    if match:
                        full_title = match.group(1).strip()
                        
                        # Split artist and title
                        if " - " in full_title:
                            parts = full_title.split(" - ", 1)
                            artist, title = parts[0].strip(), parts[1].strip()
                        elif ": " in full_title:
                            parts = full_title.split(": ", 1)
                            artist, title = parts[0].strip(), parts[1].strip()
                        elif "|" in full_title:
                            parts = full_title.split("|", 1)
                            artist, title = parts[0].strip(), parts[1].strip()
                        else:
                            artist, title = "Unknown", full_title
                        
                        # Clean up prefixes
                        if title.startswith("By "):
                            title = title[3:]
                        if artist.startswith("By "):
                            artist = artist[3:]
                        
                        print(f"‚úÖ {station['name']}: {artist} - {title}")
                        
                        return {
                            "station": station["name"],
                            "frequency": station["frequency"],
                            "artist": artist[:100],
                            "title": title[:200],
                            "timestamp": datetime.utcnow().isoformat(),
                            "region": station["region"],
                            "source_url": station["url"]
                        }
                    else:
                        print(f"‚ö†Ô∏è {station['name']}: No song found")
                        
        except asyncio.TimeoutError:
            print(f"‚è±Ô∏è {station['name']}: Timeout after 10s")
        except Exception as e:
            print(f"‚ùå {station['name']}: {str(e)[:100]}")
        
        return None
    
    async def scrape_all(self):
        """Scrape all stations"""
        print(f"üìª Scraping {len(self.stations)} Ugandan radio stations...")
        
        tasks = []
        for station in self.stations:
            tasks.append(self.scrape_one(station))
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filter successful results
        songs = []
        for result in results:
            if isinstance(result, Exception):
                continue
            if result:
                songs.append(result)
        
        return songs

async def main():
    """Main scraping function"""
    print("üöÄ UG Board Radio Scraper - GitHub Actions")
    print(f"‚è∞ Started at: {datetime.utcnow().isoformat()}")
    print("-" * 50)
    
    scraper = UgandaRadioScraper()
    songs = await scraper.scrape_all()
    
    print("-" * 50)
    print(f"üìä Results: {len(songs)} songs found")
    
    if songs:
        # Send to UG Board Engine API
        import requests
        
        payload = {
            "items": songs,
            "source": "radio",
            "timestamp": datetime.utcnow().isoformat(),
            "metadata": {
                "runner": "github-actions",
                "scrape_id": hashlib.md5(str(datetime.now()).encode()).hexdigest()[:16],
                "action_run_id": os.environ.get('GITHUB_RUN_ID', 'unknown'),
                "stations_scraped": len(set(s["station"] for s in songs))
            }
        }
        
        try:
            print("üì§ Sending to UG Board Engine API...")
            
            response = requests.post(
                "https://ugboard-engine.onrender.com/ingest/radio",
                json=payload,
                headers={
                    "Content-Type": "application/json",
                    "X-Internal-Token": "1994199620002019866"
                },
                timeout=15
            )
            
            print(f"üì® API Response: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json()
                print(f"‚úÖ Success! Items processed: {result.get('items_processed', 0)}")
                print(f"üìù Message: {result.get('message', 'No message')}")
                
                # Save summary for next steps
                summary = {
                    "songs_found": len(songs),
                    "api_success": True,
                    "items_processed": result.get("items_processed", 0),
                    "stations": list(set(s["station"] for s in songs)),
                    "timestamp": datetime.utcnow().isoformat()
                }
                
                # Write to file for workflow summary
                with open("scrape_summary.json", "w") as f:
                    json.dump(summary, f)
                    
                # Also output as GitHub Actions step output
                print(f"::set-output name=songs_found::{len(songs)}")
                print(f"::set-output name=api_success::true")
                
            else:
                print(f"‚ùå API Error: {response.status_code}")
                print(f"Response: {response.text[:200]}")
                print(f"::set-output name=api_success::false")
                
        except Exception as e:
            print(f"‚ùå Failed to send to API: {e}")
            print(f"::set-output name=api_success::false")
    else:
        print("‚ÑπÔ∏è No songs found this time")
        print(f"::set-output name=songs_found::0")
    
    print("=" * 50)
    print("üéâ Scraping completed!")

if __name__ == "__main__":
    asyncio.run(main())
EOF
        
        # Run the scraper
        python radio_scraper.py
        
    - name: Upload scrape summary
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: radio-scrape-summary
        path: |
          scrape_summary.json
          radio_scraper.py
        retention-days: 7
        
    - name: Post to GitHub summary
      if: always()
      run: |
        echo "## üìª Radio Scrape Results" >> $GITHUB_STEP_SUMMARY
        
        if [ -f scrape_summary.json ]; then
          SONGS_FOUND=$(jq -r '.songs_found' scrape_summary.json)
          API_SUCCESS=$(jq -r '.api_success' scrape_summary.json)
          STATIONS=$(jq -r '.stations | join(", ")' scrape_summary.json)
          TIMESTAMP=$(jq -r '.timestamp' scrape_summary.json)
          
          if [ "$API_SUCCESS" = "true" ]; then
            echo "‚úÖ **SUCCESS**" >> $GITHUB_STEP_SUMMARY
            echo "Found **$SONGS_FOUND** songs" >> $GITHUB_STEP_SUMMARY
            echo "Stations: $STATIONS" >> $GITHUB_STEP_SUMMARY
            echo "Time: $TIMESTAMP" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå **PARTIAL FAILURE**" >> $GITHUB_STEP_SUMMARY
            echo "Found **$SONGS_FOUND** songs but API call failed" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "‚ùå **SCRAPER FAILED**" >> $GITHUB_STEP_SUMMARY
          echo "No summary file generated" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üîÑ Next Run" >> $GITHUB_STEP_SUMMARY
        echo "The scraper will run again in **30 minutes**" >> $GITHUB_STEP_SUMMARY
        
    - name: Send success notification (optional)
      if: success()
      run: |
        echo "‚úÖ Scraping completed successfully!"
        echo "Check the logs for details."
        
    - name: Send failure notification (optional)
      if: failure()
      run: |
        echo "‚ùå Scraping failed!"
        echo "Check the logs for errors."
