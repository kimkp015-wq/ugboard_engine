"""
streams_scraper.py - Enhanced with Playwright for JavaScript-heavy sites
"""

import asyncio
import json
import re
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import logging

# Standard scraping libraries
import requests
from bs4 import BeautifulSoup
from rapidfuzz import fuzz, process

# Playwright for JavaScript-heavy sites
from playwright.async_api import async_playwright

# Use existing logger
logger = logging.getLogger("streams_scraper")

@dataclass
class ScrapedSong:
    """Normalized song data structure"""
    title: str
    artist: str
    plays: Optional[int] = None
    score: float = 0.0
    rank: Optional[int] = None
    source_type: str = "streaming"
    source: str = ""
    url: Optional[str] = None
    region: str = "central"
    metadata: Optional[Dict] = None

class StreamsScraper:
    """Production-ready streams scraper with Playwright support"""
    
    def __init__(self, db_service=None, config=None, use_playwright: bool = True):
        self.db = db_service
        self.config = config
        self.use_playwright = use_playwright
        self.playwright_browser = None
        
        # Headers for mobile simulation
        self.headers = {
            "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1"
        }
        
        # Platform configurations with scraping method
        self.platforms = {
            "songboost": {
                "name": "SongBoost Radio",
                "url": "https://charts.songboost.app/",
                "enabled": True,
                "weight": 1.5,
                "timeout": 15,
                "retries": 2,
                "method": "requests",  # Simple HTML
                "selectors": {
                    "container": ".chart-item, .chart-track, table tbody tr",
                    "title": ".song-title, .track-name, td:nth-child(2)",
                    "artist": ".artist-name, .singer, td:nth-child(3)"
                }
            },
            "spotify": {
                "name": "Spotify Uganda",
                "url": "https://open.spotify.com/embed/playlist/37i9dQZEVXbLuVZhVkCJ64",  # Uganda Top 50
                "enabled": True,
                "weight": 1.0,
                "timeout": 30,
                "retries": 2,
                "method": "playwright",  # JavaScript-heavy
                "selectors": {
                    "container": '[data-testid="tracklist-row"], .tracklist-row',
                    "title": '[data-testid="tracklist-title"], .track-name',
                    "artist": '[data-testid="tracklist-artist"], .artist-name'
                }
            },
            "boomplay": {
                "name": "Boomplay Uganda",
                "url": "https://www.boomplay.com/charts/4849",  # Uganda Top 100
                "enabled": True,
                "weight": 0.8,
                "timeout": 25,
                "retries": 2,
                "method": "playwright",  # Dynamic content
                "selectors": {
                    "container": '.chartItem, .song-item, .music-item',
                    "title": '.song-name, .title, .name',
                    "artist": '.artist-name, .singer, .artist'
                }
            },
            "audiomack": {
                "name": "Audiomack Uganda",
                "url": "https://audiomack.com/world/country/uganda",
                "enabled": True,
                "weight": 0.6,
                "timeout": 20,
                "retries": 2,
                "method": "playwright",  # JavaScript content
                "selectors": {
                    "container": 'article, .music-card, .chart-item',
                    "title": '.music-obj__title, .title, .song-title',
                    "artist": '.music-obj__artist, .artist, .singer'
                }
            }
        }
        
        # Common Ugandan artist name variations
        self.ugandan_artists_patterns = [
            r"(?i)bobi\s+wine", r"(?i)eddy\s+kenzo", r"(?i)sheebah",
            r"(?i)azawi", r"(?i)vinka", r"(?i)alien\s+skin",
            r"(?i)spice\s+diana", r"(?i)rema", r"(?i)winnie\s+nwagi",
            r"(?i)jose\s+chameleone", r"(?i)bebe\s+cool", r"(?i)pallaso",
            r"(?i)daddy\s+andre", r"(?i)geosteady", r"(?i)fik\s+fameica",
            r"(?i)john\s+blaq", r"(?i)vyroota", r"(?i)vivian\s+tosh",
            r"(?i)king\s+saha", r"(?i)david\s+lutalo", r"(?i)zex\s+bilangilangi"
        ]
        
        # Known Ugandan artists for fuzzy matching
        self.known_ugandan_artists = [
            "Bobi Wine", "Eddy Kenzo", "Sheebah", "Azawi", "Vinka",
            "Alien Skin", "Spice Diana", "Rema", "Winnie Nwagi",
            "Jose Chameleone", "Bebe Cool", "Pallaso", "Daddy Andre",
            "Geosteady", "Fik Fameica", "John Blaq", "Vyroota",
            "Vivian Tosh", "King Saha", "David Lutalo", "Zex Bilangilangi",
            "B2C", "Chosen Becky", "Karole Kasita", "Ray G", "Truth 256"
        ]
    
    async def _init_playwright(self):
        """Initialize Playwright browser if not already initialized"""
        if not self.playwright_browser and self.use_playwright:
            try:
                playwright = await async_playwright().start()
                self.playwright_browser = await playwright.chromium.launch(
                    headless=True,
                    args=[
                        '--disable-blink-features=AutomationControlled',
                        '--disable-dev-shm-usage',
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-web-security',
                        '--disable-features=IsolateOrigins,site-per-process'
                    ]
                )
                logger.info("âœ… Playwright browser initialized")
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Playwright: {e}")
                self.playwright_browser = None
        
        return self.playwright_browser
    
    async def _close_playwright(self):
        """Close Playwright browser"""
        if self.playwright_browser:
            await self.playwright_browser.close()
            self.playwright_browser = None
            logger.info("âœ… Playwright browser closed")
    
    def _clean_string(self, text: str) -> str:
        """Clean and normalize text strings"""
        if not text:
            return ""
        
        # Remove common prefixes/suffixes and normalize
        text = re.sub(r'\(.*?\)|\[.*?\]|\{.*?\}', '', text)  # Remove parentheses content
        text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
        text = text.split('ft.')[0].split('Feat.')[0].split('featuring')[0]
        text = text.split(' x ')[0].split(' X ')[0]  # Remove collaborations
        text = text.replace('"', '').replace("'", "")
        
        # Remove common suffixes
        suffixes = ['official video', 'official audio', 'lyrics', 'video', 'audio', 'visualizer']
        for suffix in suffixes:
            if text.lower().endswith(suffix):
                text = text[:-len(suffix)].strip()
        
        return text.strip()
    
    def _extract_artist_title(self, raw_text: str) -> Tuple[str, str]:
        """Extract artist and title from raw text using multiple strategies"""
        if not raw_text:
            return "Various Artists", "Unknown"
        
        clean_text = self._clean_string(raw_text)
        
        # Strategy 1: Pattern matching
        patterns = [
            (r'^(.*?)\s*[â€“â€”\-~]\s*(.*)$', (0, 1)),  # Artist - Title
            (r'^(.*?)\s+by\s+(.*)$', (1, 0)),       # Title by Artist
            (r'^(.*?)\s*:\s*(.*)$', (0, 1)),        # Artist: Title
            (r'^(.*?)\s*\|\s*(.*)$', (0, 1)),       # Artist | Title
        ]
        
        for pattern, indices in patterns:
            match = re.match(pattern, clean_text)
            if match:
                artist_idx, title_idx = indices
                artist = match.group(artist_idx + 1).strip()
                title = match.group(title_idx + 1).strip()
                
                # Additional cleaning
                artist = re.sub(r'(?i)ft\.|feat\.|featuring|&|,.*', '', artist).strip()
                title = re.sub(r'(?i)ft\.|feat\.|featuring|&|,.*', '', title).strip()
                
                return artist, title
        
        # Strategy 2: Fuzzy matching with known artists
        best_match, score, _ = process.extractOne(
            clean_text,
            self.known_ugandan_artists,
            scorer=fuzz.partial_ratio
        )
        
        if score > 70:  # Good match
            # Extract title by removing artist name
            title = clean_text.replace(best_match, '').strip(' -:')
            if title:
                return best_match, title
        
        # Strategy 3: Assume first word(s) are artist, rest is title
        words = clean_text.split()
        if len(words) >= 2:
            # Try to find where title might start (common pattern)
            for i in range(1, len(words)):
                # Check if current word looks like end of artist name
                if words[i] in ['-', '~', 'â€”'] or words[i-1] in ['by', 'By']:
                    artist = ' '.join(words[:i]).strip('- ~â€”')
                    title = ' '.join(words[i+1:]).strip()
                    if artist and title:
                        return artist, title
        
        # Final fallback: Assume it's just a title
        return "Various Artists", clean_text
    
    def _is_ugandan_artist(self, artist: str) -> bool:
        """Check if artist is Ugandan using multiple methods"""
        if not artist:
            return False
        
        artist_lower = artist.lower()
        
        # Method 1: Pattern matching
        for pattern in self.ugandan_artists_patterns:
            if re.search(pattern, artist_lower):
                return True
        
        # Method 2: Fuzzy matching with known artists
        if len(artist) > 3:  # Avoid matching very short strings
            best_match, score, _ = process.extractOne(
                artist,
                self.known_ugandan_artists,
                scorer=fuzz.token_sort_ratio
            )
            if score > 75:  # Good match threshold
                return True
        
        # Method 3: Keywords
        ugandan_keywords = ['uganda', 'kampala', 'ugandan', 'mziki wa']
        if any(keyword in artist_lower for keyword in ugandan_keywords):
            return True
        
        return False
    
    def _calculate_score(self, rank: int, platform: str, total_items: int = 50) -> float:
        """Calculate unified score based on rank and platform weight"""
        platform_config = self.platforms.get(platform, {"weight": 1.0})
        weight = platform_config.get("weight", 1.0)
        
        # Higher rank = higher score (rank 1 is best)
        normalized_rank = max(1, rank)
        base_score = 100 - ((normalized_rank - 1) / total_items * 80)
        
        # Apply platform weight
        weighted_score = base_score * weight
        
        return round(weighted_score, 2)
    
    async def _scrape_with_playwright(self, platform: str) -> List[ScrapedSong]:
        """Scrape JavaScript-heavy websites using Playwright"""
        songs = []
        platform_config = self.platforms[platform]
        
        browser = await self._init_playwright()
        if not browser:
            logger.error(f"âŒ Playwright not available for {platform}, falling back")
            return self._get_fallback_data(platform)
        
        try:
            # Create a new context with mobile emulation
            context = await browser.new_context(
                viewport={'width': 375, 'height': 667},  # iPhone size
                user_agent=self.headers["User-Agent"],
                locale='en-US',
                timezone_id='Africa/Kampala'
            )
            
            page = await context.new_page()
            
            # Set up request interception to block unnecessary resources
            await page.route("**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2,ttf,eot}", lambda route: route.abort())
            await page.route("**/*.css", lambda route: route.abort())
            
            logger.info(f"ðŸŒ Loading {platform_config['name']} with Playwright...")
            
            # Navigate to URL with timeout
            await page.goto(
                platform_config["url"],
                wait_until="networkidle",  # Wait for network to be idle
                timeout=platform_config["timeout"] * 1000
            )
            
            # Wait for content to load
            await page.wait_for_timeout(2000)  # Additional wait
            
            # Platform-specific scraping logic
            if platform == "spotify":
                songs = await self._parse_spotify(page, platform_config)
            elif platform == "boomplay":
                songs = await self._parse_boomplay(page, platform_config)
            elif platform == "audiomack":
                songs = await self._parse_audiomack(page, platform_config)
            
            await context.close()
            
            logger.info(f"âœ… {platform}: Found {len(songs)} songs with Playwright")
            
        except Exception as e:
            logger.error(f"âŒ Playwright scraping failed for {platform}: {e}")
            songs = self._get_fallback_data(platform)
        
        return songs
    
    async def _parse_spotify(self, page, config: Dict) -> List[ScrapedSong]:
        """Parse Spotify playlist page"""
        songs = []
        
        try:
            # Try multiple selectors for track rows
            selectors = config["selectors"]["container"].split(', ')
            
            for selector in selectors:
                track_elements = await page.query_selector_all(selector)
                if track_elements:
                    break
            
            for i, element in enumerate(track_elements[:50], 1):
                try:
                    # Extract title
                    title_elem = await element.query_selector(config["selectors"]["title"])
                    title = await title_elem.text_content() if title_elem else ""
                    
                    # Extract artist
                    artist_elem = await element.query_selector(config["selectors"]["artist"])
                    artist = await artist_elem.text_content() if artist_elem else ""
                    
                    if title and artist:
                        title = self._clean_string(title)
                        artist = self._clean_string(artist)
                        
                        if self._is_ugandan_artist(artist):
                            songs.append(ScrapedSong(
                                title=title,
                                artist=artist,
                                rank=i,
                                score=self._calculate_score(i, "spotify"),
                                source_type="streaming",
                                source="stream_spotify",
                                metadata={
                                    "platform": config["name"],
                                    "scraped_at": datetime.utcnow().isoformat(),
                                    "method": "playwright"
                                }
                            ))
                except Exception as e:
                    logger.debug(f"Error parsing Spotify track {i}: {e}")
            
            # If no tracks found via selectors, try to extract from page text
            if not songs:
                page_text = await page.text_content()
                # Simple pattern matching for song titles
                # This is a fallback and may need refinement
                pass
                
        except Exception as e:
            logger.error(f"Error parsing Spotify: {e}")
        
        return songs
    
    async def _parse_boomplay(self, page, config: Dict) -> List[ScrapedSong]:
        """Parse Boomplay charts page"""
        songs = []
        
        try:
            # Wait for chart items to load
            await page.wait_for_selector(config["selectors"]["container"], timeout=10000)
            
            # Get all chart items
            items = await page.query_selector_all(config["selectors"]["container"])
            
            for i, item in enumerate(items[:50], 1):
                try:
                    # Extract title and artist
                    title_elem = await item.query_selector(config["selectors"]["title"])
                    artist_elem = await item.query_selector(config["selectors"]["artist"])
                    
                    if title_elem and artist_elem:
                        title = await title_elem.text_content()
                        artist = await artist_elem.text_content()
                        
                        title = self._clean_string(title)
                        artist = self._clean_string(artist)
                        
                        if self._is_ugandan_artist(artist):
                            songs.append(ScrapedSong(
                                title=title,
                                artist=artist,
                                rank=i,
                                score=self._calculate_score(i, "boomplay"),
                                source_type="streaming",
                                source="stream_boomplay",
                                metadata={
                                    "platform": config["name"],
                                    "scraped_at": datetime.utcnow().isoformat(),
                                    "method": "playwright"
                                }
                            ))
                except Exception as e:
                    logger.debug(f"Error parsing Boomplay item {i}: {e}")
        
        except Exception as e:
            logger.error(f"Error parsing Boomplay: {e}")
        
        return songs
    
    async def _parse_audiomack(self, page, config: Dict) -> List[ScrapedSong]:
        """Parse Audiomack charts page"""
        songs = []
        
        try:
            # Wait for content to load
            await page.wait_for_selector(config["selectors"]["container"], timeout=10000)
            
            # Get all music items
            items = await page.query_selector_all(config["selectors"]["container"])
            
            for i, item in enumerate(items[:50], 1):
                try:
                    # Extract title and artist
                    title_elem = await item.query_selector(config["selectors"]["title"])
                    artist_elem = await item.query_selector(config["selectors"]["artist"])
                    
                    if title_elem and artist_elem:
                        title = await title_elem.text_content()
                        artist = await artist_elem.text_content()
                        
                        title = self._clean_string(title)
                        artist = self._clean_string(artist)
                        
                        if self._is_ugandan_artist(artist):
                            songs.append(ScrapedSong(
                                title=title,
                                artist=artist,
                                rank=i,
                                score=self._calculate_score(i, "audiomack"),
                                source_type="streaming",
                                source="stream_audiomack",
                                metadata={
                                    "platform": config["name"],
                                    "scraped_at": datetime.utcnow().isoformat(),
                                    "method": "playwright"
                                }
                            ))
                except Exception as e:
                    logger.debug(f"Error parsing Audiomack item {i}: {e}")
        
        except Exception as e:
            logger.error(f"Error parsing Audiomack: {e}")
        
        return songs
    
    def _scrape_with_requests(self, platform: str) -> List[ScrapedSong]:
        """Scrape simple HTML websites using requests"""
        songs = []
        platform_config = self.platforms[platform]
        
        try:
            response = requests.get(
                platform_config["url"],
                headers=self.headers,
                timeout=platform_config["timeout"],
                allow_redirects=True
            )
            
            if response.status_code != 200:
                logger.error(f"HTTP {response.status_code} from {platform}")
                return self._get_fallback_data(platform)
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Platform-specific parsing for requests-based scraping
            if platform == "songboost":
                songs = self._parse_songboost_html(soup, platform_config)
            else:
                # Generic parsing for other platforms
                songs = self._parse_generic_html(soup, platform_config, platform)
            
            logger.info(f"âœ… {platform}: Found {len(songs)} songs with requests")
            
        except Exception as e:
            logger.error(f"âŒ Requests scraping failed for {platform}: {e}")
            songs = self._get_fallback_data(platform)
        
        return songs
    
    def _parse_songboost_html(self, soup, config: Dict) -> List[ScrapedSong]:
        """Parse SongBoost HTML"""
        songs = []
        
        selectors = config["selectors"]["container"].split(', ')
        items = []
        
        for selector in selectors:
            items = soup.select(selector)
            if items:
                break
        
        for i, item in enumerate(items[:50], 1):
            try:
                text = item.get_text(separator=' ', strip=True)
                artist, title = self._extract_artist_title(text)
                
                if title != "Unknown" and self._is_ugandan_artist(artist):
                    songs.append(ScrapedSong(
                        title=title,
                        artist=artist,
                        rank=i,
                        score=self._calculate_score(i, "songboost"),
                        source_type="radio",
                        source="stream_songboost",
                        metadata={
                            "platform": config["name"],
                            "scraped_at": datetime.utcnow().isoformat(),
                            "method": "requests",
                            "raw_text": text[:100]
                        }
                    ))
            except Exception as e:
                logger.debug(f"Error parsing SongBoost item {i}: {e}")
        
        return songs
    
    def _parse_generic_html(self, soup, config: Dict, platform: str) -> List[ScrapedSong]:
        """Generic HTML parsing for requests-based platforms"""
        songs = []
        
        selectors = config["selectors"]["container"].split(', ')
        items = []
        
        for selector in selectors:
            items = soup.select(selector)
            if items:
                break
        
        for i, item in enumerate(items[:50], 1):
            try:
                title_elem = item.select_one(config["selectors"]["title"])
                artist_elem = item.select_one(config["selectors"]["artist"])
                
                if title_elem and artist_elem:
                    title = self._clean_string(title_elem.get_text(strip=True))
                    artist = self._clean_string(artist_elem.get_text(strip=True))
                    
                    if title and artist and self._is_ugandan_artist(artist):
                        songs.append(ScrapedSong(
                            title=title,
                            artist=artist,
                            rank=i,
                            score=self._calculate_score(i, platform),
                            source_type="streaming",
                            source=f"stream_{platform}",
                            metadata={
                                "platform": config["name"],
                                "scraped_at": datetime.utcnow().isoformat(),
                                "method": "requests"
                            }
                        ))
            except Exception as e:
                logger.debug(f"Error parsing {platform} item {i}: {e}")
        
        return songs
    
    def _get_fallback_data(self, platform: str) -> List[ScrapedSong]:
        """Generate fallback data when scraping fails"""
        logger.warning(f"Using fallback data for {platform}")
        
        # Sample Ugandan songs for fallback
        ugandan_songs = [
            ("Nalumansi", "Bobi Wine"),
            ("Sitya Loss", "Eddy Kenzo"),
            ("Malaika", "Sheebah"),
            ("Bomboclat", "Alien Skin"),
            ("Number One", "Azawi"),
            ("Sweet Love", "Vinka"),
            ("Tonjola", "Spice Diana"),
            ("Kaddugala", "Winnie Nwagi"),
            ("Biri Biri", "John Blaq"),
            ("Tubonga Naawe", "Various Artists")
        ]
        
        platform_config = self.platforms.get(platform, {"name": platform})
        
        songs = []
        for i, (title, artist) in enumerate(ugandan_songs[:10], 1):
            songs.append(ScrapedSong(
                title=title,
                artist=artist,
                rank=i,
                score=self._calculate_score(i, platform),
                source_type="streaming",
                source=f"stream_{platform}",
                metadata={
                    "platform": platform_config["name"],
                    "scraped_at": datetime.utcnow().isoformat(),
                    "fallback": True
                }
            ))
        
        return songs
    
    async def scrape_platform(self, platform: str) -> List[ScrapedSong]:
        """Scrape a single platform using appropriate method"""
        if platform not in self.platforms:
            logger.error(f"Unknown platform: {platform}")
            return []
        
        platform_config = self.platforms[platform]
        
        if not platform_config.get("enabled", True):
            logger.info(f"Platform {platform} is disabled")
            return []
        
        logger.info(f"ðŸŽµ Scraping {platform_config['name']}...")
        
        # Choose scraping method
        method = platform_config.get("method", "requests")
        
        if method == "playwright" and self.use_playwright:
            return await self._scrape_with_playwright(platform)
        else:
            # Run requests-based scraping in thread pool for async compatibility
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(self._scrape_with_requests, platform)
                return future.result(timeout=platform_config["timeout"] + 5)
    
    async def scrape_all_async(self) -> Dict[str, Any]:
        """Scrape all platforms asynchronously"""
        start_time = time.time()
        results = {}
        
        logger.info("ðŸš€ Starting async streams scraping for all platforms")
        
        # Scrape each platform concurrently
        platforms_to_scrape = [
            p for p, config in self.platforms.items() 
            if config.get("enabled", True)
        ]
        
        # Create tasks for all platforms
        tasks = []
        for platform in platforms_to_scrape:
            task = asyncio.create_task(self._scrape_and_save_platform(platform))
            tasks.append((platform, task))
        
        # Wait for all tasks to complete
        for platform, task in tasks:
            try:
                result = await task
                results[platform] = result
            except Exception as e:
                logger.error(f"Task failed for {platform}: {e}")
                results[platform] = {
                    "status": "error",
                    "error": str(e)
                }
        
        total_time = time.time() - start_time
        
        # Record in scraper history
        if self.db:
            total_found = sum(r.get("songs_found", 0) for r in results.values() if isinstance(r, dict))
            total_added = sum(r.get("songs_saved", {}).get("added", 0) for r in results.values() if isinstance(r, dict))
            
            self.db.add_scraper_history(
                scraper_type="streams",
                station_id="all_platforms",
                items_found=total_found,
                items_added=total_added,
                status="success" if any(r.get("status") == "success" for r in results.values()) else "error",
                execution_time=total_time
            )
        
        # Clean up Playwright
        await self._close_playwright()
        
        return {
            "status": "completed",
            "scraper_type": "streams",
            "timestamp": datetime.utcnow().isoformat(),
            "total_time": round(total_time, 2),
            "platforms_scraped": len(platforms_to_scrape),
            "results": results
        }
    
    async def _scrape_and_save_platform(self, platform: str) -> Dict[str, Any]:
        """Scrape a platform and save results"""
        platform_start = time.time()
        
        try:
            # Scrape the platform
            songs = await self.scrape_platform(platform)
            
            # Save to database
            save_result = self.save_to_database(songs)
            
            platform_time = time.time() - platform_start
            
            return {
                "status": "success",
                "songs_found": len(songs),
                "songs_saved": save_result,
                "execution_time": round(platform_time, 2)
            }
            
        except Exception as e:
            platform_time = time.time() - platform_start
            logger.error(f"âŒ {platform} scraping failed: {e}")
            
            return {
                "status": "error",
                "error": str(e),
                "execution_time": round(platform_time, 2)
            }
    
    def scrape_all_sync(self) -> Dict[str, Any]:
        """Synchronous version for compatibility"""
        # Run async function in event loop
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return loop.run_until_complete(self.scrape_all_async())
    
    def save_to_database(self, songs: List[ScrapedSong]) -> Dict[str, int]:
        """Save scraped songs to database"""
        if not self.db:
            logger.warning("No database service available, skipping save")
            return {"total": len(songs), "added": 0, "updated": 0}
        
        added = 0
        updated = 0
        
        for song in songs:
            try:
                # Convert to database schema
                song_data = {
                    "title": song.title,
                    "artist": song.artist,
                    "plays": song.plays or 0,
                    "score": song.score,
                    "station": song.metadata.get("platform", f"Stream: {song.source}") if song.metadata else f"Stream: {song.source}",
                    "region": song.region,
                    "source_type": song.source_type,
                    "source": song.source,
                    "url": song.url
                }
                
                # Add to database
                was_updated, song_id = self.db.add_song(song_data)
                
                if was_updated:
                    updated += 1
                else:
                    added += 1
                    
            except Exception as e:
                logger.error(f"Failed to save song {song.title}: {e}")
        
        return {
            "total": len(songs),
            "added": added,
            "updated": updated,
            "failed": len(songs) - (added + updated)
        }
